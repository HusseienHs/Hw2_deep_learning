{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1044635a7c33d70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD now: C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\n",
      "sys.path[0]: C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\n",
      "CWD set to C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\n",
      "TRAIN_CSV exists: True\n",
      "FILES_DIRECTORY exists: True\n",
      "Example CSVs: ['0.csv', '1.csv', '10.csv', '100.csv', '1000.csv']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Move one level up to Source Code\n",
    "os.chdir(r\"C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\")\n",
    "\n",
    "# Optional: make sure Python can see this folder\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "print(\"CWD now:\", os.getcwd())\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import os, sys\n",
    "\n",
    "# Add Source Code folder to sys.path\n",
    "BASE_DIR = r\"C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\"\n",
    "\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "# Optional: change working directory (recommended)\n",
    "if os.getcwd() != BASE_DIR:\n",
    "    os.chdir(BASE_DIR)\n",
    "\n",
    "# Now you can import setup_paths\n",
    "import setup_paths  # this will run the code in setup_paths.py\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Base data folder\n",
    "DATA_DIR = r\"C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\\data\"\n",
    "\n",
    "# Path to labeled train.csv\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "\n",
    "# Path to folder containing all raw CSV files\n",
    "FILES_DIRECTORY = os.path.join(DATA_DIR, \"unlabeled\", \"unlabeled\")\n",
    "\n",
    "# Check\n",
    "print(\"TRAIN_CSV exists:\", os.path.exists(TRAIN_CSV))\n",
    "print(\"FILES_DIRECTORY exists:\", os.path.exists(FILES_DIRECTORY))\n",
    "print(\"Example CSVs:\", os.listdir(FILES_DIRECTORY)[:5])\n",
    "\n",
    "\n",
    "\n",
    "from models_utils.GLOBALS import files_directory\n",
    "from RF_XGB.RandomForest import get_rf_data\n",
    "from models_utils.utils import convert_to_features\n",
    "from models_utils.Datasets import *\n",
    "from models_utils.GLOBALS import *\n",
    "import torch.nn.functional as F\n",
    "from CNN.CNN import MultivariateCNN\n",
    "from CNN.cnn_utils import train_cnn, get_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f0cf28027bd353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T20:54:34.202543300Z",
     "start_time": "2024-02-19T20:54:34.165377100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get train data\n",
    "train_data = pd.read_csv(\n",
    "    r\"C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\\data\\train.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "train_data['activity'] = train_data['activity'].map(activity_id_mapping)\n",
    "data_type_1 = train_data[train_data['sensor'] == 'vicon'].reset_index()\n",
    "data_type_2 = train_data[train_data['sensor'] == 'smartwatch'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1de3b0f44ead8b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sizes of padding/cutting\n",
    "target_size_type1 = 3000\n",
    "target_size_type2 = 1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a5f2d600460357",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train or load models\n",
    "train_or_load_autoencoders = 'load'\n",
    "if train_or_load_autoencoders == 'train':\n",
    "    model_CNN_type1 = train_cnn('Type1OnlyCNN', data_type_1, '1', target_size_type1, 30, 64, 0.001)\n",
    "    model_CNN_type2 = train_cnn('Type2OnlyCNN', data_type_2, '2', target_size_type2, 20, 64, 0.001)\n",
    "elif train_or_load_autoencoders == 'load':\n",
    "    model_CNN_type1 = MultivariateCNN(3, target_size_type1, 18).to(device)\n",
    "    model_CNN_type1.load_state_dict(torch.load('Type1OnlyCNN.pth'))\n",
    "    model_CNN_type2 = MultivariateCNN(3, target_size_type2, 18).to(device)\n",
    "    model_CNN_type2.load_state_dict(torch.load(f'Type2OnlyCNN.pth'))\n",
    "else:\n",
    "    raise ValueError('Wrong train or load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a61cf3ec20af066",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic target sizes: 4000 1350\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from CNN.CNN import MultivariateCNN\n",
    "from CNN.cnn_utils import get_train_data\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "type1_max_len = 0\n",
    "type2_max_len = 0\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    file_path = f\"{files_directory}/{row['id']}.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    if data.shape[1] == 3:\n",
    "        type1_max_len = max(type1_max_len, len(data))\n",
    "    else:\n",
    "        acc_data = data[data.iloc[:, 0] == 'acceleration [m/s/s]'].iloc[:, 1:]\n",
    "        type2_max_len = max(type2_max_len, len(acc_data))\n",
    "\n",
    "print(\"Dynamic target sizes:\", type1_max_len, type2_max_len)\n",
    "\n",
    "model_CNN_type1 = MultivariateCNN(\n",
    "    num_channels=3,\n",
    "    input_length=type1_max_len,\n",
    "    num_classes=18\n",
    ").to(device)\n",
    "\n",
    "model_CNN_type2 = MultivariateCNN(\n",
    "    num_channels=3,\n",
    "    input_length=type2_max_len,\n",
    "    num_classes=18\n",
    ").to(device)\n",
    "\n",
    "calculate_or_load_train_data = 'load'\n",
    "\n",
    "if calculate_or_load_train_data == 'calculate':\n",
    "    data_type_1, data_type_2 = get_train_data(\n",
    "        savename='train_Only_CNN',\n",
    "        modelType1=model_CNN_type1,\n",
    "        modelType2=model_CNN_type2,\n",
    "        target_size_type1=type1_max_len,\n",
    "        target_size_type2=type2_max_len,\n",
    "        embedding_size=18,\n",
    "        is_autoencoder=False\n",
    "    )\n",
    "elif calculate_or_load_train_data == 'load':\n",
    "    data_type_1 = pd.read_csv('train_Only_CNN_type1.csv')\n",
    "    data_type_2 = pd.read_csv('train_Only_CNN_type2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4b578d0e4f4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T23:47:49.042127200Z",
     "start_time": "2024-02-16T23:35:50.155064800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get and save results for test data\n",
    "results_list = []\n",
    "sample_submission_path = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "for i, file_id in enumerate(pd.read_csv(sample_submission_path)['sample_id'].to_list()):\n",
    "\n",
    "    class_path = os.path.join(files_directory, f\"{file_id}.csv\")\n",
    "    new_data = pd.read_csv(class_path)\n",
    "\n",
    "    if new_data.shape[1] == 3:\n",
    "        if len(new_data) < 4000:\n",
    "            new_data = pad_sequence(new_data, 4000)\n",
    "        new_data = torch.tensor(new_data.values, dtype=torch.float32).to(device)\n",
    "        normalized_new_data = (new_data - min_values_type1) / (max_values_type1 - min_values_type1 + 1e-6)\n",
    "        normalized_new_data = normalized_new_data.transpose(0, 1)\n",
    "        logits = model_CNN_type1(normalized_new_data)\n",
    "\n",
    "\n",
    "    else:\n",
    "        new_data = new_data[new_data.iloc[:, 0] == 'acceleration [m/s/s]'].iloc[:, 1:]\n",
    "        if len(new_data) < 1350:\n",
    "            new_data = pad_sequence(new_data, 1350)\n",
    "        new_data = torch.tensor(new_data.values, dtype=torch.float32).to(device)\n",
    "        normalized_new_data = (new_data - min_values_type2) / (max_values_type2 - min_values_type2 + 1e-6)\n",
    "        normalized_new_data = normalized_new_data.transpose(0, 1)\n",
    "\n",
    "        logits = model_CNN_type2(normalized_new_data)\n",
    "\n",
    "    predictions = F.softmax(logits, dim=1)\n",
    "    res_dict = {activity: predictions.squeeze()[id].item() for id, activity in id_activity_mapping.items()}\n",
    "\n",
    "    result_dict = {label: res_dict.get(label, 0) for label in activity_id_mapping.keys()}\n",
    "    result_dict['sample_id'] = file_id\n",
    "    results_list.append(result_dict)\n",
    "results = pd.DataFrame(results_list, columns=['sample_id'] + list(activity_id_mapping.keys()))\n",
    "results.fillna(0).to_csv('results_raw_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load results CSV\n",
    "results = pd.read_csv('results_raw_cnn.csv')\n",
    "\n",
    "# Suppose you have true labels for train/val in train_df\n",
    "# Map 'activity' to one-hot for comparison\n",
    "activity_cols = [c for c in results.columns if c != 'sample_id']\n",
    "true_labels = train_df.set_index('id')['activity'].map(activity_id_mapping)  # numeric mapping\n",
    "true_labels_onehot = pd.get_dummies(true_labels)\n",
    "\n",
    "# Merge predicted probabilities with true labels\n",
    "# Only keep samples that exist in results\n",
    "common_ids = results['sample_id'].isin(true_labels_onehot.index)\n",
    "merged_results = results[common_ids].copy()\n",
    "merged_results['true_activity'] = [true_labels_onehot.loc[i].idxmax() for i in merged_results['sample_id']]\n",
    "\n",
    "# Calculate predicted class\n",
    "merged_results['predicted_activity'] = merged_results[activity_cols].idxmax(axis=1)\n",
    "\n",
    "# Identify good vs bad predictions\n",
    "merged_results['correct'] = merged_results['predicted_activity'] == merged_results['true_activity']\n",
    "\n",
    "# 1. Plot examples of good predictions\n",
    "good_preds = merged_results[merged_results['correct']].sample(min(5, len(merged_results[merged_results['correct']])))\n",
    "\n",
    "for _, row in good_preds.iterrows():\n",
    "    probs = row[activity_cols].values\n",
    "    top_classes = [activity_cols[i] for i in np.argsort(probs)[-5:][::-1]]\n",
    "    top_probs = np.sort(probs)[-5:][::-1]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=top_classes, y=top_probs, palette='Greens')\n",
    "    plt.title(f\"GOOD Prediction: True={row['true_activity']}, Predicted={row['predicted_activity']}\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "# 2. Plot examples of bad predictions\n",
    "bad_preds = merged_results[~merged_results['correct']].sample(min(5, len(merged_results[~merged_results['correct']])))\n",
    "for _, row in bad_preds.iterrows():\n",
    "    probs = row[activity_cols].values\n",
    "    top_classes = [activity_cols[i] for i in np.argsort(probs)[-5:][::-1]]\n",
    "    top_probs = np.sort(probs)[-5:][::-1]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=top_classes, y=top_probs, palette='Reds')\n",
    "    plt.title(f\"BAD Prediction: True={row['true_activity']}, Predicted={row['predicted_activity']}\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "# 3. Optionally, show model uncertainty: max probability histogram\n",
    "max_probs = merged_results[activity_cols].max(axis=1)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(max_probs, bins=20, kde=True)\n",
    "plt.title(\"Model confidence for all samples\")\n",
    "plt.xlabel(\"Max probability\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
