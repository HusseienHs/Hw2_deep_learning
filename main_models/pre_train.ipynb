{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc960e4",
   "metadata": {},
   "source": [
    "# Section (e) — Fine-tune a pretrained Time-Series Foundation Model (MOMENT) on HAR\n",
    "\n",
    "This notebook fine-tunes **MOMENT** (AutonLab/MOMENT-1-*) for **18-class activity classification** on your dataset, **separately for each sensor type** (Smartwatch vs Vicon), matching your existing project structure.\n",
    "\n",
    "It follows your project’s key assumptions:\n",
    "- Input is multivariate 3-axis signal (x,y,z)\n",
    "- Padding/cutting to fixed lengths (you used **3000** for Type1 and **1169** for Type2 in `only_1Dcnn.ipynb`)\n",
    "- Min/max normalization using `models_utils/GLOBALS.py`.\n",
    "\n",
    "---\n",
    "## 0) One-time setup (IMPORTANT)\n",
    "\n",
    "Your current `models_utils/GLOBALS.py` has **Windows absolute paths**. Update these two lines so they point to your local repo:\n",
    "- `BASE_DIR`\n",
    "- `files_directory`\n",
    "\n",
    "Then restart the kernel and run again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea816710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_CSV exists: True\n",
      "FILES_DIRECTORY exists: True\n",
      "Example CSVs: ['0.csv', '1.csv', '10.csv', '100.csv', '1000.csv']\n",
      "Using TRAIN_CSV: C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\\data\\train.csv\n",
      "Using files_directory: C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\\data\\unlabeled\\unlabeled\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Add Source Code folder to sys.path\n",
    "BASE_DIR = r\"C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\"\n",
    "\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "# Optional: change working directory (recommended)\n",
    "if os.getcwd() != BASE_DIR:\n",
    "    os.chdir(BASE_DIR)\n",
    "\n",
    "# Now you can import setup_paths\n",
    "import setup_paths  # this will run the code in setup_paths.py\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Base data folder\n",
    "DATA_DIR = r\"C:\\Users\\husseien\\Desktop\\340915149_322754953\\Source Code\\data\"\n",
    "\n",
    "# Path to labeled train.csv\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "\n",
    "# Path to folder containing all raw CSV files\n",
    "FILES_DIRECTORY = os.path.join(DATA_DIR, \"unlabeled\", \"unlabeled\")\n",
    "\n",
    "# Check\n",
    "print(\"TRAIN_CSV exists:\", os.path.exists(TRAIN_CSV))\n",
    "print(\"FILES_DIRECTORY exists:\", os.path.exists(FILES_DIRECTORY))\n",
    "print(\"Example CSVs:\", os.listdir(FILES_DIRECTORY)[:5])\n",
    "\n",
    "# ===============================\n",
    "# 1) Imports + paths\n",
    "# ===============================\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# If this notebook is inside your repo, set PROJECT_ROOT accordingly.\n",
    "# Example: PROJECT_ROOT = Path(r\"C:\\Users\\...\\Source Code\")\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Ensure your repo modules are importable\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# ---- Import your project globals AFTER you fix GLOBALS.py paths ----\n",
    "from models_utils.GLOBALS import (\n",
    "    device,\n",
    "    TRAIN_CSV,\n",
    "    files_directory,\n",
    "    min_values_type1, max_values_type1,\n",
    "    min_values_type2, max_values_type2,\n",
    "    activity_id_mapping, id_activity_mapping\n",
    ")\n",
    "\n",
    "from models_utils.Datasets import TrainDataframeWithLabels\n",
    "print(\"Using TRAIN_CSV:\", TRAIN_CSV)\n",
    "print(\"Using files_directory:\", files_directory)\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2007fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLASSES: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 2) Install & load MOMENT\n",
    "# ===============================\n",
    "# MOMENT model card / usage: https://huggingface.co/AutonLab/MOMENT-1-large\n",
    "# Install (run once):\n",
    "# !pip install -q momentfm\n",
    "\n",
    "from momentfm import MOMENTPipeline\n",
    "\n",
    "def build_moment_classifier(num_classes: int, n_channels: int = 3, model_name: str = \"AutonLab/MOMENT-1-small\"):\n",
    "    \"\"\"Create a MOMENT classification pipeline and initialize it.\"\"\"\n",
    "    model = MOMENTPipeline.from_pretrained(\n",
    "        model_name,\n",
    "        model_kwargs={\n",
    "            \"task_name\": \"classification\",\n",
    "            \"n_channels\": n_channels,\n",
    "            \"num_class\": num_classes,\n",
    "        },\n",
    "    )\n",
    "    model.init()\n",
    "    return model\n",
    "\n",
    "NUM_CLASSES = len(activity_id_mapping)  # should be 18\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7bc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows smartwatch: 36186\n",
      "Rows vicon: 14062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 3) Data split exactly like your CNN training (stratified 80/20)\n",
    "# ===============================\n",
    "train_df = pd.read_csv(TRAIN_CSV).reset_index(drop=True)\n",
    "\n",
    "# Your dataset has two sensor types; in your code, Type1 vs Type2 is determined by file shape.\n",
    "# In practice, your train.csv already has 'sensor' column, so we reuse it.\n",
    "smartwatch_df = train_df[train_df[\"sensor\"] == \"smartwatch\"].reset_index(drop=True)\n",
    "vicon_df      = train_df[train_df[\"sensor\"] == \"vicon\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Rows smartwatch:\", len(smartwatch_df))\n",
    "print(\"Rows vicon:\", len(vicon_df))\n",
    "\n",
    "# Match your only_1Dcnn.ipynb constants\n",
    "TARGET_SIZE_TYPE1 = 3000\n",
    "TARGET_SIZE_TYPE2 = 1169\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a26aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_split(df: pd.DataFrame, test_size=0.2, seed=42):\n",
    "    labels = df[\"activity\"].tolist()\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(df)),\n",
    "        test_size=test_size,\n",
    "        stratify=labels,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    return list(train_idx), list(val_idx)\n",
    "\n",
    "smart_train_idx, smart_val_idx = make_split(smartwatch_df, test_size=0.2, seed=42)\n",
    "vic_train_idx, vic_val_idx     = make_split(vicon_df, test_size=0.2, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb7273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartwatch batches: 905 227\n",
      "Vicon batches: 352 88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 4) Datasets + loaders\n",
    "# ===============================\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0  # set >0 if your OS supports it well\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "\n",
    "smartwatch_dataset = TrainDataframeWithLabels(smartwatch_df, data_type=\"2\", max_sequence_length=TARGET_SIZE_TYPE2)\n",
    "vicon_dataset      = TrainDataframeWithLabels(vicon_df,      data_type=\"1\", max_sequence_length=TARGET_SIZE_TYPE1)\n",
    "\n",
    "smart_train_loader = DataLoader(Subset(smartwatch_dataset, smart_train_idx), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "smart_val_loader   = DataLoader(Subset(smartwatch_dataset, smart_val_idx), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                               num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "vic_train_loader = DataLoader(Subset(vicon_dataset, vic_train_idx), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "vic_val_loader   = DataLoader(Subset(vicon_dataset, vic_val_idx), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"Smartwatch batches:\", len(smart_train_loader), len(smart_val_loader))\n",
    "print(\"Vicon batches:\", len(vic_train_loader), len(vic_val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6915fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5) Normalization helpers (same as your code)\n",
    "# ===============================\n",
    "def normalize_batch(x: torch.Tensor, data_type: str) -> torch.Tensor:\n",
    "    \"\"\"x shape: (bs, seq_len, 3)\"\"\"\n",
    "    if data_type == \"1\":\n",
    "        return (x - min_values_type1) / (max_values_type1 - min_values_type1 + 1e-6)\n",
    "    else:\n",
    "        return (x - min_values_type2) / (max_values_type2 - min_values_type2 + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc8b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 6) Training loop for MOMENT classification\n",
    "# ===============================\n",
    "def run_epoch(model, loader, data_type: str, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"train\" if is_train else \"val\", leave=False)\n",
    "    for x, y in pbar:\n",
    "        # Your dataset returns x as (seq_len, 3) per sample, so batch is (bs, seq_len, 3)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        x = normalize_batch(x, data_type)\n",
    "\n",
    "        # MOMENT expects (bs, seq_len, channels) for classification\n",
    "        # If your momentfm version expects a different layout, this is the ONLY line to adjust.\n",
    "        logits = model(x).logits  # (bs, num_classes)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total += y.size(0)\n",
    "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "        pbar.set_postfix(loss=total_loss / max(total, 1), acc=100 * correct / max(total, 1))\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def finetune_moment(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    data_type: str,\n",
    "    model_name: str,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 3e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    freeze_backbone_epochs: int = 2,\n",
    "    save_path: str = \"moment_finetuned.pth\",\n",
    "):\n",
    "    model = build_moment_classifier(num_classes=NUM_CLASSES, n_channels=3, model_name=model_name)\n",
    "    model.to(device)\n",
    "\n",
    "    # Two-phase training:\n",
    "    # (1) linear-probe (freeze backbone) to stabilize\n",
    "    # (2) unfreeze and fine-tune end-to-end\n",
    "    def set_backbone_trainable(trainable: bool):\n",
    "        # momentfm pipelines usually expose .model; we try to freeze everything except the classification head\n",
    "        # If your version differs, print(model) and adjust these names.\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = trainable\n",
    "        # Try to keep classification head trainable if we can detect it\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"head\" in name.lower() or \"classifier\" in name.lower():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    # Phase 1: freeze\n",
    "    set_backbone_trainable(False)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if epoch == freeze_backbone_epochs + 1:\n",
    "            set_backbone_trainable(True)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, data_type=data_type, optimizer=optimizer)\n",
    "        val_loss, val_acc     = run_epoch(model, val_loader,   data_type=data_type, optimizer=None)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train loss {train_loss:.4f} acc {train_acc*100:.2f}% | \"\n",
    "              f\"val loss {val_loss:.4f} acc {val_acc*100:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(best_state, save_path)\n",
    "            print(f\"  ✅ saved best to: {save_path} (val acc {best_val_acc*100:.2f}%)\")\n",
    "\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ed48be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning on SMARTWATCH (Type2, len=1169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\momentfm\\models\\moment.py:174: UserWarning: Only reconstruction head is pre-trained. Classification and forecasting heads must be fine-tuned.\n",
      "  warnings.warn(\"Only reconstruction head is pre-trained. Classification and forecasting heads must be fine-tuned.\")\n",
      "                                              \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m MODEL_NAME = \u001b[33m\"\u001b[39m\u001b[33mAutonLab/MOMENT-1-small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFine-tuning on SMARTWATCH (Type2, len=1169)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m best_smart = \u001b[43mfinetune_moment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43msmart_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43msmart_val_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_backbone_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmoment_smartwatch_best.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFine-tuning on VICON (Type1, len=3000)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m best_vicon = finetune_moment(\n\u001b[32m     22\u001b[39m     train_loader=vic_train_loader,\n\u001b[32m     23\u001b[39m     val_loader=vic_val_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     save_path=\u001b[33m\"\u001b[39m\u001b[33mmoment_vicon_best.pth\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mfinetune_moment\u001b[39m\u001b[34m(train_loader, val_loader, data_type, model_name, epochs, lr, weight_decay, freeze_backbone_epochs, save_path)\u001b[39m\n\u001b[32m     77\u001b[39m     set_backbone_trainable(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     78\u001b[39m     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m train_loss, train_acc = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m val_loss, val_acc     = run_epoch(model, val_loader,   data_type=data_type, optimizer=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, loader, data_type, optimizer)\u001b[39m\n\u001b[32m     11\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     13\u001b[39m pbar = tqdm(loader, desc=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Your dataset returns x as (seq_len, 3) per sample, so batch is (bs, seq_len, 3)\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\husseien\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\340915149_322754953\\Source Code\\models_utils\\Datasets.py:58\u001b[39m, in \u001b[36mTrainDataframeWithLabels.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     55\u001b[39m     x = x[:\u001b[38;5;28mself\u001b[39m.max_sequence_length]\n\u001b[32m     57\u001b[39m x = torch.tensor(x.values, dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m y = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mactivity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[31mTypeError\u001b[39m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 7) Run fine-tuning (choose model size)\n",
    "# ===============================\n",
    "# Options: \"AutonLab/MOMENT-1-small\", \"AutonLab/MOMENT-1-base\", \"AutonLab/MOMENT-1-large\"\n",
    "# Start with small (faster), then try base if you have GPU memory.\n",
    "MODEL_NAME = \"AutonLab/MOMENT-1-small\"\n",
    "\n",
    "print(\"Fine-tuning on SMARTWATCH (Type2, len=1169)\")\n",
    "best_smart = finetune_moment(\n",
    "    train_loader=smart_train_loader,\n",
    "    val_loader=smart_val_loader,\n",
    "    data_type=\"2\",\n",
    "    model_name=MODEL_NAME,\n",
    "    epochs=10,\n",
    "    lr=3e-5,\n",
    "    freeze_backbone_epochs=2,\n",
    "    save_path=\"moment_smartwatch_best.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuning on VICON (Type1, len=3000)\")\n",
    "best_vicon = finetune_moment(\n",
    "    train_loader=vic_train_loader,\n",
    "    val_loader=vic_val_loader,\n",
    "    data_type=\"1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    epochs=10,\n",
    "    lr=3e-5,\n",
    "    freeze_backbone_epochs=2,\n",
    "    save_path=\"moment_vicon_best.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\nBest val accuracies:\")\n",
    "print(\"  smartwatch:\", best_smart)\n",
    "print(\"  vicon     :\", best_vicon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ebcc0",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Compare to sections (c–d)\n",
    "\n",
    "To compare fairly, use the **same split** (the stratified indices computed in this notebook) and report:\n",
    "- Validation Accuracy (or Macro-F1 if you used it in c–d)\n",
    "- Training vs Validation curves (loss/acc)\n",
    "\n",
    "**What to report in the writeup (Section e):**\n",
    "1. **Baseline (c–d)**: paste the best validation accuracy you got with your best CNN/LSTM model.\n",
    "2. **Pretrained MOMENT**:\n",
    "   - Smartwatch best val acc = `best_smart`\n",
    "   - Vicon best val acc      = `best_vicon`\n",
    "3. A short explanation:\n",
    "   - MOMENT should help especially when labels are limited or classes are imbalanced (pretraining gives better representations).\n",
    "   - If it underperforms your CNN, likely reasons are: small batch size, not enough epochs, or mismatch of expected input layout.\n",
    "\n",
    "If you want, we can add:\n",
    "- confusion matrix\n",
    "- macro-F1\n",
    "- a submission generator (like your `get_results` flow)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
